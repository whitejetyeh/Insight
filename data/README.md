The training images are first processed by blender python api. In data/blender_snapshot.py, I rendered snapshots of the [3D facial model](http://www.micc.unifi.it/masi/research/ffd/) spinned at various angles, and then I resized the grayscale snapshots to (width,height)=(64,64). Then, in LRsymmetrizer.py, feed_processor stacks up the frontal image with the corresponding side image. By the assumption of symmetric face from left to right, I stacked the front view with the left view when the head is facing left, and vice versa. The pair of images is the training data for the convolutional autoencoder.
